# 决策树

决策树模型是一种对实例进行**分类/回归**的**树形结构**。可以看作一个`if-then`规则集合（根结点到叶结点的每一条路径代表规则，内部节点的特征代表规则的条件，叶结点的类对应规则的结论）。

决策树还可以表示给定特征条件下**类的条件概率分布**，即将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210315185439891.png" alt="image-20210315185439891" style="zoom:50%;" />

**学习目标**：给定训练数据集构建一个决策树模型，使它能够对实例进行正确的分类，即归纳出一组分类规则。具体来说

1. 与训练数据矛盾较小的决策树；
2. 具有很好的泛化性；

目标的衡量采用**损失函数**，正则化的极大似然函数；

在损失函数的定义下从所有的可能中选取最优决策树是一个NP完全问题。通常学习算法采用**启发式方法**近似求解这一最优化问题【次优】。

决策树算法主要**包含三个步骤**：

1. 特征选择；
2. 决策树的生成；
3. 剪枝（具有更好的泛化能力）；



## 1. 特征选择

**特征选择的目的**是对训练数据集选取当前最具有分类能力的特征。对于分类决策树来说，特征选择的准则有：

1. 信息增益；
2. 信息增益比；
3. 基尼指数；

### 1.1 信息增益

信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。【**ID3算法**选择信息增益作为特征选择准则】

首先介绍熵的内容，熵表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，概率分布：
$$
P\left(X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n
$$
随机变量$X$的熵定义为：
$$
H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
若对于随机变量$(X, Y)$，联合概率分布为：
$$
    P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, n ; \quad j=1,2, \cdots, m
$$
条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为Y的条件概率分布的熵对X的数学期望：
$$
H(Y \mid X)=\sum_{i=1}^{n} p_{i} H\left(Y \mid X=x_{i}\right)
$$

信息增益定义如下：
> 特征$A$对训练数据集D的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：
> $$
g(D, A)=H(D)-H(D \mid A)
> $$
> 两者之差在信息论中被称为**互信息**，信息增益等价于训练数据集类与特征之间的互信息。

具体的，经验熵$H(D)$：
$$
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$
经验条件熵$H(D|A)$：
$$
H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
$$
其中，$|C_k|$表示属于类$C_k$的样本个数，$|D_i|$表示子集$D_i$的个数（由特征$A$进行划分出的）。



### 1.2 信息增益比

信息增益**偏向于选择取值较多的特征**，因此需要对其进行校正。因此，信息增益比为信息增益的基础上与数据集$D$关于特征$A$的值的熵$H_A(D)$之比，【**C4.5**算法选择信息增益比】即：
$$
g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}
$$
其中，$H_{A}(D)=-\displaystyle\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}$，$n$是特征A取值的个数。

### 1.3 基尼指数

分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数为
$$
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
$$
对于二分类问题，属于第一个类的概率为$p$，基尼指数为：【两个类别】
$$
\operatorname{Gini}(p)=2 p(1-p)
$$
基尼指数越大，样本集合的不确定性也越大。

## 2. 决策树生成

经典的决策树生成算法主要有ID3、C4.5和CART。

### 2.1 ID3

ID3算法定义如下：
> 输入：训练数据集$D$，特征集$A$，阈值$\varepsilon$；  
> 输出：决策树$T$。  
> （1）若$D$中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该结点的类标记，返回$T$；  
> （2）若$A=\varnothing$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；  
> （3）否则，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；  
> （4）如果$A_g$的信息增益小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；  
> （5）否则，对$A_g$的每一可能值$a_i$，依$A_g=a$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；  
> （6）对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用步（1）~步（5），得到子树$T_i$，返回$T$；

ID3算法的缺点：

- 信息增益准则【对可取值数目较多的特征有所偏好】，类似“编号”的特征其信息增益接近于 1；
- 【只能用于处理离散分布的特征】；【无法处理连续值】
- 【没有考虑缺失值】；
- 只有树的生成，【无剪枝】，故算法生成的树【容易过拟合】；



### 2.2 C4.5

采用信息增益比来选择特征，其他与ID3算法相似。

信息增益比对可取值数目较少的属性有所偏好。C4.5并不是直接选择增益率最大的候选划分属性，而是选择一个启发式：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的**。

C4.5算法相对于ID3算法的缺点对应有以下改进方式：

- 【引入信息增益率】作为划分标准；
- 将【连续特征离散化】；
- 引入【悲观剪枝策略进行后剪枝】；
- 对于【缺失值进行了处理】；

C4.5的缺点：

- C4.5 用的是【多叉树】，用二叉树效率更高；
- C4.5 【只能用于分类】；
- C4.5 使用的熵模型拥有大量耗时的【对数运算】，【连续值还有排序运算】；
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。



### 2.3 CART

CART，分类与回归树，假设决策树是**二叉树**，因此决策树的生成就是递归地构建二叉树。对回归用平方误差最小化准则，对分类树用基尼指数最小化准则。  

**最小回归树生成算法**【重要】

> 输入：训练数据集$D$。  
> 输出：回归树$f(x)$。  
> 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉树：  
> （1）**对输入空间进行划分**。采用启发式的方法，选择切分变量$j$与切分点$s$，定义两个区域：
> $$
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\} \quad \text { 和 } \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\}
> $$
> 然后寻找最优切分变量$j$（**特征选择**）和最优切分点$s$，求解
> $$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
> $$
> $c_1,c_2$为固定的输出值【一般为均值】，遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的$(j,s)$。  
> （2）用选定的对$(j,s)$划分区域并决定相应的输出值：
> $$
\begin{array}{c}R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}, \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\} \\ \hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2\end{array}
> $$
> （3）继续对两个子区域调用步骤（1）（2），直至满足条件。  
> （4）将输入空间划分为$M$个区域$R_1,R_2,...,R_M$，生成决策树：
> $$
f(x)=\sum_{m=1}^{M} \hat{c}_{m} I\left(x \in R_{m}\right)
> $$

**分类树生成**

> 输入：训练数据集$D$，停止计算的条件；  
> 输出：CART决策树；  
> 根据训练数据集，从根结点开始，递归地对每个结点进行以下操作：  
> （1）设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数。对每一个特征$A$，对其可能取得每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_1$和$D_2$两部分，计算$A=a$的基尼指数；    
> （2）在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。  
> （3）对两个子结点递归地调用（1）（2），直至满足停止条件；  
> （4）生成CART决策树。  
> 算法停止计算的条件是结点中的样本个数小于预定阈值，或基尼指数小于预定阈值，或没有更多特征。



## 3. 剪枝

**预剪枝与后剪枝**：

- 预剪枝：在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；【基于贪心，带来欠拟合的风险】
- 后剪枝：先从训练集生成一颗完整的决策树，然后自底向上对非叶结点考察，若将该结点对应的子树替换为叶结点带来决策树泛化性能提升，则将该子树替换为叶结点；【训练开销大】

决策树生成算法递归地产生决策树，容易过拟合（学习时过多地考虑如何提高对训练数据的正确分类，从而构建出复杂的决策树）。解决的办法就是对已生成的决策树进行简化，即**剪枝**。

决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过**优化损失函数还考虑了减小模型复杂度**。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。

一般的剪枝算法过程为：
> 输入：生成算法产生的整个树$T$，参数$\alpha$；  
> 输出：修剪后的子树$T$。  
> （1）计算每个结点的经验熵。  
> （2）递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别是$C_{\alpha}(T_B)$与$C_{\alpha}(T_A)$，如果
> $$
> C_{\alpha}\left(T_{A}\right) \leqslant C_{\alpha}\left(T_{B}\right)
> $$
> 则将父结点进行剪枝，变为新的叶结点；  
> （3）返回（2），直至不能继续位置，得到损失函数最小的子树$T_{\alpha}$。

**CART剪枝**

CART剪枝由两步组成：

1. 从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\{T_0,T_1,...,T_n\}$
2. 通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树；

> 输入：CART算法生成的决策树$T_0$；  
> 输出：最优决策树$T_{\alpha}$；  
> （1）设$k=0,T=T_0$；  
> （2）设$\alpha=+\infty$；  
> （3）自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及
> $$
\begin{aligned} g(t) &=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1} \\ \alpha &=\min (\alpha, g(t)) \end{aligned}
> $$
> $T_t$表示以$t$为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。  
> （4）对$g(t)=\alpha$的内部结点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$；  
> （5）设$k=k+1,\alpha_k=\alpha,T_k=T$；  
> （6）如果$T_k$不是由根结点及两个叶结点构成的树，则回到（2）；否则令$T_k=T_n$；  
> （7）采用交叉验证法在子树序列中选取最优子树$T_\alpha$；



## 4. ID3、C4.5、CART的差异

- 特征选择的标准：ID3使用了信息增益，会倾向于取值较多的特征，C4.5对ID3进行了优化，使用信息增益率来对取值数目多的特征进行惩罚，避免出现过拟合的特性，提升决策树的泛化能力。而CART进一步使用Gini指数作为判别标准，减少了大量的对数运算，提高了算法的执行效率。
- 样本类型：ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。C4.5通过二分法（见下文），对于CART，由于其构建时每次都会对特征进行二值划分，因此可以很好的适用于连续型变量。
- 应用方面：ID3和C4.5只能用于分类任务，CART还可以应用于回归任务。
- 缺失值处理：ID3对缺失值比较敏感，C4.5和CART对缺失值有不同的处理方式。
- 剪枝策略的差异：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。



## 5. 连续值与缺失值的处理

### 5.1 连续值处理

连续属性的可取值数目不再有限，因此需要**连续属性离散化技术**。

**C4.5：**

在C4.5决策树算法中**采用了二分法对连续属性进行处理**。

假设给定样本集$D$和连续特征$A$，$A$在$D$上有$n$个不同的取值，将这些值从小到大进行排序，并取相邻两样本值的平均数共$n-1$个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点。

且与离散属性不同，若当前结点划分属性为连续属性，则该属性还可以作为后代结点的划分属性。(见对鸢尾花分类问题构建决策树)

例如在鸢尾花分类问题中，有如下样本(1～3为山鸢尾，4～6为变色鸢尾，7～9安德森鸢尾)：

```
[5.1 3.5 1.4 0.2]
[4.9 3.  1.4 0.2]
[4.7 3.2 1.3 0.2]
[7.  3.2 4.7 1.4]
[6.4 3.2 4.5 1.5]
[6.9 3.1 4.9 1.5]
[6.5 3.  5.5 1.8]
[7.7 3.8 6.7 2.2]
[7.7 2.6 6.9 2.3]
```

我们首先选择第一列花萼长度进行排序：

```
4.7, 4.9, 5.1, 6.4, 6.5, 6.9, 7., 7.7, 7.7
```

产生8个划分点（划分点的值为相邻的平均树）：

```
4.8, 5.0, 5.75, 6.45, 6.7, 6.95, 7.35, 7.7
```

其中$H(D)=-(\frac{1}{3}log\frac{1}{3}+\frac{1}{3}log\frac{1}{3}+\frac{1}{3}log\frac{1}{3})=1.586$

每个点的信息增益分别为：$1.586-（-\frac{1}{9}*0-\frac{8}{9}*(\frac{1}{4}log\frac{1}{4}+\frac{3}{8}log\frac{3}{8}+\frac{3}{8}log\frac{3}{8}))=0.192$，0.453，0.913, 0.680, 0.680, 0.913, 0.453, 0.4816, 0.192，选择最大值之一5.75作为划分点。再依次对对其他的特征进行计算信息增益。



### 5.2 缺失值处理

现实任务中会经常遇到不完整样本，即样本的某些属性会缺失。故需要解决两个问题：

1. 如何在属性值缺失的情况下进行划分属性选择？（即如何计算特征的信息增益率（C4.5）或基尼指数（CART））
2. 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

C4.5和CART决策树算法分别给出了不同的解决方法。

#### C4.5处理缺失值

针对问题1，C4.5的做法是：对于具有缺失值特征，**用没有缺失的样本子集所占比重来折算**；假设$\rho$表示无缺失值样本所占的比例，$\hat{p}_k$表示无缺失值中第$k$类所占的比例，$\hat{r}_v$表示无缺失值样本中在特征$A$上取值$A^v$的样本所占的比例。故可将信息增益推广为：
$$
Gain(D,A)=\rho \times Gain(\hat{D},A)=\rho \times (Ent(\hat{D})-\displaystyle\sum^{V}_{v=1}\hat{r}_vEnt(\hat{D}^v))
$$
其中
$$
Ent(\hat{D})=-\displaystyle\sum^{|Y|}_{k=1}\hat{p}_klog_2\hat{p}_k​
$$
针对问题2，C4.5的做法是：若样本$x$在划分特征$A$上的取值已知，则将$x$划入对应取值的子结点，且样本权值在子结点中保持为$w_x$【初始值为1】。**若取值未知，则将样本同时划分到所有子节点，不过要调整样本的权重值**，其实也就是以不同概率划分到不同节点中。

#### CART处理缺失值

对于问题 1，CART 一开始严格要求分裂特征评估时【只能使用在该特征上没有缺失值的那部分数据】，在后续版本中，CART 算法【使用了一种惩罚机制来抑制提升值】，从而反映出缺失值的影响（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。

对于问题 2，CART 算法的机制是为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。【在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征）】，当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是【决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点】。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。