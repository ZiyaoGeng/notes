# LightGBM

XGBoost与LightGBM的比较。

![image-20210313133916508](https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313133916508.png)

LightGBM相对于XGBoost作出了如下优化：

- 基于Histogram的决策树算法。
- 单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以**减少大量只具有小梯度的数据实例**，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。
- 互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。
- 带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。
- 直接支持类别特征(Categorical Feature)
- 支持高效并行
- Cache命中率优化



## LightGBM基本原理

### 1. 基于直方图的决策树算法

对于GBDT来说，主要代价是学习决策树，而学习决策树最耗时的部分是**寻找最佳分裂点**。当时有两种算法：

1. **预排序算法**：每个特征的值需要经过排序才方便枚举所有可能的分割点。该算法简单，但在**训练速度和内存消耗方面效率低下**。
2. **基于直方图（Histogram）的算法**：基于直方图的算法不是在排序后的特征值上寻找分裂点，而是将每一个特征的连续的特征值放入离散的容器中，并在训练过程中使用这些容器来构造特征直方图。基于直方图的算法在内存消耗和训练速度上都更有效。

LightGBM应用了直方图算法，如下图所示：

![image-20210313145822222](https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313145822222.png)

基于直方图的算法有以下两个优点：

- 内存占用更小：**不需要存储排序后的结果**（32位浮点型），也**不需要保存映射的索引**（排序前->排序后，32位整型），只保存**特征离散化后的值**（8位整型），因此内存消耗降低为原来的$$1/8$$。

  <img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313151325169.png" alt="image-20210313151325169" style="zoom:50%;" />

- 计算代价更小：在寻找分割点时，对于GBDT来说，时间复杂度是O(#data × #feature)，而对于LightGBM来说，为O(#bin × #feature)。并且$$\#bin <<\#data$$，所以能够加快训练

【注】基于直方图的算法并不是在LightGBM中提出，LightGBM只是对其进行了应用。

所以基于直方图的GBDT算法：

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313134159683.png" alt="image-20210313134159683" style="zoom: 25%;" />

另外基于直方图的算法还可以利用直方图做差加速。叶子的直方图可以由它父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313152506500.png" alt="image-20210313152506500" style="zoom: 25%;" />



### 2. 带有最大深度限制的Leaf-wise算法

大多数GBDT算法（包括XGBoost）都采用基于level-wise的算法。简单来说就是按层进行分裂树的增长策略，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上**很多叶子的分裂增益较低，没必要进行搜索和分裂**，因此带来了很多没必要的计算开销。

LightGBM采用Leaf-wise的增长策略，该策略每次**从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂**，如此循环。因此同Level-wise相比，

- Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；
- Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个**最大深度的限制，在保证高效率的同时防止过拟合**。

如下图所示：

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313153232113.png" alt="image-20210313153232113" style="zoom:25%;" />



### 3. GOSS

当数据量足够大的时候，GBDT算法的复杂度就很很高，**减小样本量**和减小特征维度（数量）是两个考虑的方向。但如何对样本进行采样？【如果直接随机丢弃的话，会造成数据分布与原来不同的问题】。因此，LightGBM提出了GOSS（Gradient-based One-Side Sampling，基于梯度的单边采样算法）。

作者提到，在AdaBoost中，样本权重可以很好地反映数据实例的重要性。然而，在GBDT中，没有原生的样本权值，因此针对AdaBoost提出的采样方法不能直接应用。**但是在GBDT中每一个数据（样本）都有不同的梯度值（回想一下，我们计算损失的时候，总是是将所有损失累计，因此每个样本都有对应的损失值），对于损失值小的样本，说明模型已经学得很好了，但直接丢弃不可取（前面说了，会造成数据分布的改变）**，所以作者采用了一种采样方式GOSS。

GOSS首先将要进行分裂的特征的所有取值**按照绝对值大小**降序排序（不需要保存，因为目的是丢弃部分数据），然后选取前百分之$$a$$个数据保留。剩下的数据量为$$(1-a)*\#data$$，再随机选取百分之$$b$$的数据【这里的$$b$$是对应于整个样本的比例，见算法2，这个很关键$$a+b<=1$$】。**GOSS在计算信息增益时**，将采样数据以小的梯度乘以常数$$\frac{1-a}{b}$$。通过这样做，我们可以将更多的注意力放在训练不足的实例上，而不会对原始数据分布做太多的改变。

算法如下所示：

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313155431222.png" alt="image-20210313155431222" style="zoom: 50%;" />

- $$a$$为大梯度值的数据的采样比例，$$b$$为小梯度值的采样比例，$$d$$为迭代次数；
- 在迭代中，首先模型计算对每个样本的预测值，然后计算损失得到梯度$$g$$；然后按梯度的绝对值进行排序得到排序后的索引。
  - 取样本的百分之$$a$$作为梯度值最大的样本集；
  - 再对剩下的样本随机取**占整个数据集的百分之$$b$$**作为梯度小的样本集。
  - 在计算增益时，随机取的样本集需要乘上$$\frac{1-a}{b}$$，保证了之前剩下样本集的增益保持不变。

举个例子，假设有100样本，$$a=0.5$$，$$b=0.2$$，则随机取的为剩下的50个中的20样本。简单期间，增益原来就为50，那么目前剩下的$$20*\frac{0.5}{0.2}=50$$。



### 4. EFB

之前讲到，除了减少样本的数量，还有个方向就是来减少特征的数量，即作者提出了EFB（Exclusive Feature Bundling，互斥捆绑算法）。

高维数据（特征数量多）通常非常稀疏。特征空间的稀疏性提供了设计一种几乎无损的方法来减少特征数量的可能性。具体来说，在稀疏特征空间中，**许多特征是互斥的，即它们不会同时取非零值。我们可以安全地将互斥性特征捆绑到单个特征中**。通过一个特征扫描算法，可以从特征捆绑集合中构建与单个特征相同的特征直方图。这样，**直方图构建的复杂度由O(#data × #feature)变化为O(#data × #bundle)**，而#bundle << #feature。这样可以在不影响精度的前提下显著加快GBDT的训练速度。

这需要面临两个问题：

（1）确定哪些特征应该捆绑在一起；

（2）如何构建这个捆绑集合（bundle）；

**（1）**

对于第一个问题，其实是一个NP-hard问题，这表明不可能在多项式时间内找到精确解。为了找到一个良好的近似算法，作者首先将最优捆绑问题归结为图着色问题，即**以特征为顶点，如果两个特征互斥则每两个特征加边**，然后使用一个贪婪算法，可以得到较好的结果。作者观察到，虽然很多特征并不是百分之百是互斥的，但也很少同时取到非零值，因此如果允许存在一部分冲突，就可以归纳为更少的特征集合来提高计算效率。作者选择一个参数$$\gamma$$来平衡精度和效率。算法如下所示：

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313164011874.png" alt="image-20210313164011874" style="zoom:50%;" />

- 首先，构造一个带加权边的图，其**权重对应于特征之间的总的矛盾【互斥程度】**。
- 其次，将图中的特征按度降序排序。【度越大，则说明越容易和其他进行捆绑】
- 最后，检查有序列表中的每个特征，并将其分配给具有小冲突（由$$\gamma$$控制）的现有bundle，或者创建一个新的bundle。

算法的时间复杂度为$$O(\#feature^2)$$，训练前只进行一次处理。当特征的数量不是很大时，这种复杂性是可以接受的，但如果有数百万个特征，这种复杂性可能仍然会受到影响。为了进一步提高效率，**作者提出了一种不需要构建图的更有效的排序策略：按非零值的计数排序，这类似于按度排序，因为更多的非零值通常会导致更高的冲突概率**。

**（2）**

对于第二个问题，需要一个方法将特征合并到同一个bundle中，以减少相应的训练复杂度。关键是要确保原始特征的值可以从bundle中识别出来。由于基于直方图的算法存储的是离散的容器，而不是特征的连续值，因此我们可以通过**让特定的特征留在不同的容器中来构建一个特征包**。这可以通过**在特征的原始值上添加偏移量（offset）来实现**。【特征例如，假设在一个bundle中有两个特征。原来，特征A取$$[0,10)$$值，特征B取$$[0,20)$$值。然后给特征B的值添加一个10的偏移量【0，20都进行偏移了】，这样精炼的特征从$$[10,30]$$中取值。之后，可以合并特征A和B，并使用范围为$$[0,30]$$的特征包（bundle）来替换原来的特征A和B，具体为：

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313170636634.png" alt="image-20210313170636634" style="zoom:50%;" />

EFB算法可以将大量的互斥特征捆绑到数量少得多的密集特征上，有效地避免了对零特征值的不必要计算。



## 工程优化

### 1. Cache命中率优化

预排序算法所造成的cache-miss，带来系统性能的下降：

- 访问特征的梯度是随机的，不同的特征访问梯度的顺序也不同；
- 访问行索引到结点索引也是随机的

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313172344139.png" alt="image-20210313172344139" style="zoom:50%;" />

基于直方图的算法：

- 对梯度的访问，不需要对特征进行排序，只需要对梯度的访问进行一个重新的排序，就可以实现连续访问；
- 不需要数据id到叶子结点的索引表；

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313171927078.png" alt="image-20210313171927078" style="zoom:50%;" />



### 2. 支持类别特征

传统的机器学习算法不能直接输入类别特征，需要进行离散化（one-hot编码），这样会生成多维特征， 这样会降低算法的效率。

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313171550567.png" alt="image-20210313171550567" style="zoom:50%;" />





### 3. 支持并行化学习

包括：

- 特征并行化：【适用于小数据】；
- 数据并行化：【数据量特别大，特征比较少的情景】；
- 投票并行化：数据量大且特征也比较大；

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313173330381.png" alt="image-20210313173330381" style="zoom:50%;" />

特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。LightGBM 则不进行数据垂直划分，而是在每台机器上保存全部训练数据，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313173419375.png" alt="image-20210313173419375" style="zoom:50%;" />

水平切分数据：

LightGBM在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313173516399.png" alt="image-20210313173516399" style="zoom:50%;" />

对数据并行的优化：

基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。【在数据量很大的时候】，使用投票并行的方式【只合并部分特征的直方图】从而达到降低通信量的目的，可以得到非常好的加速效果。具体过程如下图所示。

大致步骤为两步：

1. 本地找出 Top K 特征，并基于投票筛选出可能是最优分割点的特征；
2. 合并时只合并每个机器选出来的特征。

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210313173537119.png" alt="image-20210313173537119" style="zoom:50%;" />

通过本地找到本地的Best特征，利用投票筛选出可能是全局最优分割点的特征，合并直方图只合并被选出来的特征，从而降低通信量。