# 感知机

感知机（perceptron）是二分类的线性分类模型，对应于输入空间中将实例划分为正负两类的分离超平面，属于**判别模型**。

**学习目的**：求出将训练数据进行线性划分的**分离超平面**。



## 模型定义

> 假设输入空间是$$\mathcal{X}\subseteq\mathbf{R}^n$$，输出空间是$$\mathcal{Y}=\{+1,-1\}$$。输入$$x \in \mathcal{X}$$表示实例的特征向量，对应于输入空间的点，输出$$y\in \mathcal{Y}$$表示实例的类别。由输入空间到输出空间的如下函数：
> $$
> f(x)=sign(w\cdot x+b)
> $$
> 称为感知机。其中$$w,b$$为模型参数，$$sign$$为符号函数。



### 几何解释

感知机有几何解释：线性方程
$$
w\cdot x+b=0
$$
对应于特征空间$$\mathbf{R}^n$$中的一个超平面$$S$$，其中$$w$$是超平面的法向量，$$b$$是超平面的截距。这个超平面将特征空间划分为两类，两部分的点分别被分为正、负两类。因此，这个超平面被称为**分离超平面**。

<img src="/Users/gengziyao/Study/Note/Machine_Learning/images/29.png" style="zoom:50%;" />



### 学习策略

假设训练集线性可分，感知机学习的目标是求得一个能够将训练集的正负点完全正确分开的分离超平面。为了确定模型参数$$w,b$$，需要确定学习策略，即定义经验损失并将其最小化。



### 损失函数

选择误分类点到超平面的总距离。

1、输入空间中任意一点$$x_0$$到超平面的距离：
$$
\frac{1}{\|w\|}|w\cdot x_0+b|
$$
其中，$$\|w\|$$是$$w$$的$$L_2$$范数。

2、对应于误分类点$$(x_i,y_i)$$来说，
$$
-y_i(w\cdot x_i+b)>0
$$
成立。故不考虑$$\frac{1}{\|w\|}$$，**感知机的损失函数**为：
$$
L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)
$$
其中$$M$$为误分类点的集合。



## 学习算法

感知机的学习算法可分为原始形式和对偶形式。  
**1、原始形式**

给定一个训练集
$$
T=\{(x_1,y_1,(x_2,y_2),...,(x_N,y_N)\}
$$
其中，$$x_i\in \mathcal{X}=\mathbf{R}^n,y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$$，求参数$$w,b$$，使其为损失函数极小化问题的解：
$$
\min_{w,b}L(w,b)=-\sum_{x_i \in M}y_i(w \cdot x_i+b)
$$
采用**随机梯度下降法**进行求解。

损失函数的梯度为：
$$
\nabla_{w} L(w, b)=-\sum_{x_{i} \in M} y_{i} x_{i} \\ \nabla_{b} L(w, b)=-\sum_{x_{i} \in M} y_{i}
$$
随机选取一个误分类点$$(x_i,y_i)$$，对$$w,b$$进行更新：
$$
w \leftarrow w+\eta y_ix_i\\
b \leftarrow b+ \eta y_i
$$
其中$$\eta$$为步长，即学习率。

因此感知机学习算法的原始形式定义为：

> 输入：训练集$$T=\{(x_1,y_1,(x_2,y_2),...,(x_N,y_N)\}$$，其中$$x_i\in \mathcal{X}=\mathbf{R}^n,y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$$
>
> 输出：$$w,b$$；感知机模型$$f(x)=sign(w\cdot x +b)$$
>
> (1) 选取初值$$w_0,b_0$$；
>
> (2) 在训练集中选取数据$$(x_i,y_i)$$；
>
> (3) 如果$$y_i(w \cdot x_i +b)\le0$$：
> $$
> w \leftarrow w+\eta y_ix_i\\
> b \leftarrow b+ \eta y_i
> $$
> (4) 转至(2) 直至训练集中没有误分类点。【条件是数据集线性可分】

学习算法有直观解释：当一个实例点被误分类，则调整$$w,b$$的值，**使分离超平面向该误分类点的一侧移动**，以减少该误分类点与超平面间的距离，直至超平面越过误分类点使其被正确分类。  
**2、对偶形式**

对偶形式的**基本思想**：将$$w$$和$$b$$表示为实例$$x_i$$和标记$$y_i$$的线性组合形式，通过求解其系数来求得$$w$$和$$b$$。

原始形式$$w,b$$关于$$(x_i,y_i)$$的**增量**分别是$$\alpha_iy_ix_i$$和$$\alpha_iy_i$$，这里$$\alpha_i=n_i\eta$$。因此，最后学习到的$$w,b$$可以表示为：
$$
w=\sum_{i=1}^N\alpha_iy_ix_i \\
b=\sum_{i=1}^N\alpha_iy_i
$$
因此学习算法的对偶形式为：

> 输入：训练集$$T=\{(x_1,y_1,(x_2,y_2),...,(x_N,y_N)\}$$，其中$$x_i\in \mathcal{X}=\mathbf{R}^n,y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$$
>
> 输出：$$\alpha,b$$；感知机模型$$f(x)=sign(\displaystyle\sum_{j=1}^{N}\alpha_jy_jx_j\cdot x+b)$$，其中$$\alpha=(\alpha_1,\alpha_2,...,\alpha_N)^T$$。
>
> (1) $$\alpha \leftarrow 0,b \leftarrow 0$$；
>
> (2) 在训练集中选择数据$$(x_i,y_i)$$；
>
> (3) 如果$$y_i(\displaystyle\sum_{j=1}^{N}\alpha_jy_jx_j \cdot x_i +b) \le0$$，
> $$
> \alpha_i \leftarrow \alpha_i + \eta \\
> b \leftarrow b + \eta y_i
> $$
> (4) 转至(2) 直至没有误分类点。

对偶形式中**训练实例仅以内积的形式出现**。因此为了方便，可以预先将内积计算进行矩阵的存储【降低运算量】，该矩阵就是$$Gram$$矩阵。
$$
\mathbf{G}=[x_i\cdot x_j]_{N \times N}
$$
【注】感知机采用对偶形式的条件是特征空间的维度远大于数据集的大小。感知机的对偶形式是为了后续支持向量机作为基础。
