# k近邻法

k近邻法是一种基本分类与回归方法（以下讨论分类），利用训练数据集对特征向量空间进行划分，通过多数表决等方式进行预测，因此不具有显示的学习过程。具有三个基本要素：**k值**、**距离度量**、**分类决策**。

## 定义

k近邻法（k-nearest neighbor，k-NN）有如下定义（分类）：

> 输入：训练数据集
> $$
> T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
> $$
> 其中，$x_i \in \mathcal{X} \subseteq \mathbf{R}^n$为实例的特征向量，$y_i \in \mathcal{Y}=\{c_1,c_2,...,c_K\}$为实例的类别，$i=1,2,...,N$；实例特征向量$x$；
>
> 输出：实例$x$所属的类$y$。
>
> （1）根据给定的距离度量，在训练集$T$中找出与$x$最邻近的$k$个点，涵盖这$k$个点的$x$的邻域记作$N_k(x)$；
>
> （2）在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$；
> $$
> y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right), \quad i=1,2, \cdots, N ; j=1,2, \cdots, K
> $$
> 其中$I$为指示函数。

## 三个基本要素

### 1. k值选择

- 较小的k值：只有与输入实例较近的训练实例才会对预测结果起作用。但预测结果会对近邻的实例点非常敏感。**k值的减小意味着整体模型变得复杂，容易发生过拟合**。
- 较大的k值：用较大的邻域中的训练实例进行预测，这时与输入实例不相似的也会对预测起作用，使预测发生错误。**k值的增大就意味着整体的模型变得简单，容易发生欠拟合**。

### 2. 距离度量

特征空间中两个实例点的距离是两个实例点**相似程度**的反映。特征空间一般是n维实数向量空间$\mathbf{R}^n$，一般是使用欧式距离，但也可以是更一般的$L_p$距离（或Minkowski距离）。

设特征空间$\mathcal{X}$是$n$维实数空间$\mathbf{R}^n$，$x_i,x_j \in \mathcal{X},x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},...,x_j^{(n)})^T$，则$x_i,x_j$的$L_p$距离定义为：
$$
L_p(x_i,x_j)=(\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
$$

- $p=1$，为曼哈顿距离；

- $p=2$，为欧式距离；

- $p=\infty$，是各个坐标距离的最大值：
  $$
  L_{\infty}\left(x_{i}, x_{j}\right)=\max _{l}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|
  $$

### 3. 分类决策规则

k近邻法中的分类决策规则往往是**多数表决**。

如果分类的损失函数为0-1损失，分类函数为
$$
f:\mathbf{R}^n \rightarrow \{c_1,c_2,...,c_K\}
$$
误分类的概率：
$$
P(Y\neq f(X))=1-P(Y=f(X))
$$
对于给定的实例$x \in \mathcal{X}$，其最近邻的k个训练实例点构成集合$N_k(x)$。如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是：
$$
\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i} \neq c_{j}\right)=1-\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right)
$$
要使误分类率最小，即经验风险最小，就要使$\sum_{x_{i} \in N_{k}(x)} I(y_{i}=c_{j})$最大。**多数表决规则等价于经验风险最小化**。

## 总结

1、k近邻模型对应于基于训练数据集对特征空间的一个划分。当训练集、距离度量、k值以及分类决策确定后，其结果唯一确定。

2、k近邻法**不具有显示的学习过程**，即无需训练。

3、k近邻法的三个基本要素：k值的选择、距离度量、分类决策规则；

4、k近邻法的实现需要考虑如何快速搜索k个最近邻点。**kd树**是一种便于对k维空间中的数据进行快速搜索的数据结构。

