# Dropout

训练一个深度神经网络时，总会遇到两个问题；

1）训练时间长；

2）过拟合问题；

那么如果当训练一个深度神经网络时，可以通过随机丢弃一部分神经元来避免过拟合，称为Dropout。

设置一个固定的概率$p$，对于每一个神经元都以概率$p$判断是否保留。一个神经层$y=f(Wx+b)$，引入一个掩蔽函数$mask(\cdot)$，使得$y=f(Wmask(x)+b)$，掩蔽函数的定义：
$$
mask(x)=\left\{\begin{array}{l}m\odot x \ 训练时\\ px \ \ \ \ \ \ \  测试时\end{array}\right.
$$
$m\in{0,1}$是丢弃掩码，【通过以概率为$p$的伯努利分布随机生成】。训练时，激活神经元的平均数量为原来的$p$倍。测试时，**所有神经元都可以激活**（不丢弃），这会造成训练和测试时网络输出不一致。为了缓解这个问题，测试时将神经层的输入$x$乘以$p$。



## 解决过拟合的原因

**（1）取平均的作用：** dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，【整个dropout过程就相当于对很多个不同的神经网络取平均】。【近似集成学习bagging】

**（2）减少神经元之间复杂的共适应关系：** 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它【不应该对一些特定的线索片段太过敏感】，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。



## 集成学习的角度

每一次做丢弃，相当于从原始的网络中采样得到一个子网络，这些子网络都共享原始网络的参数，那么最终的网络可以近似看作集成了指数级个不同网络的组合。



## 贝叶斯学习角度

用$y=f(x;\theta_m)$表示学习的神经网络，贝叶斯学习假设参数$\theta$为随机向量，且先验分布$q(\theta)$，贝叶斯方法的预测：
$$
\begin{aligned} \mathbb{E}_{q(\theta)}[y] &=\int_{q} f(\boldsymbol{x} ; \theta) q(\theta) d \theta \\ & \approx \frac{1}{M} \sum_{m=1}^{M} f\left(\boldsymbol{x}, \theta_{m}\right) \end{aligned}
$$
其中$f\left(\boldsymbol{x}, \theta_{m}\right) $为第$m$次应用丢弃方法后的网络。



## 循环神经网络的Dropout

当在循环神经网络上应用丢弃法时，不能直接对每个时刻的隐状态进行随 机丢弃，这样会损害循环网络在时间维度上的记忆能力。一种简单的方法是对 非时间维度的连接（即非循环连接）进行随机丢失。

<img src="https://gzy-gallery.oss-cn-shanghai.aliyuncs.com/img/image-20210321143940615.png" alt="image-20210321143940615" style="zoom:50%;" />