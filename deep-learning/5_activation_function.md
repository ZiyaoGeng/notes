# 激活函数

非线性函数$f(\cdot)$称为激活函数，性质：

（1）连续并可导（允许少数点不可导）的非线性函数；

（2）激活函数及其导函数尽可能简单，提高网络计算效率；

（3）激活函数的导函数的值域要在一个适合的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性；



## 1. Sigmoid函数

Sigmoid是指一类S型曲线函数，为两端饱和函数；

> 对于函数$f(x)$，若$x \rightarrow -\infty$，其导数$f'(x) \rightarrow 0$，称为左饱和。若$x \rightarrow \infty$，其导数$f'(x) \rightarrow 0$，称为右饱和。同时满足，称为两端饱和。

经典的Logistic函数：
$$
\sigma(x)=\frac{1}{1+exp(-x)}
$$
性质：

1）输出可以直接看作概率分布；

2）看作一个门控，控制其他神经元输出信息的数量；

Tanh函数：
$$
tanh(x)=\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}
$$
与Logistic的关系：
$$
tanh(x)=2\sigma(2x)-1
$$


## 2. ReLU函数

ReLU，修正线性单元：
$$
ReLU(x)=max(0, x)
$$
ReLU为左饱和函数，且在$x>0$时导数为1，一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。

缺点：输出是非零中心化的，给后一层的神经网络引入偏置会影响梯度下降的效率。



## 3. GELU函数

GELU，高斯误差线性单元，通过门控调整输入输出。
$$
GELU(x)=xP(X\leq x)
$$
其中$P(X\leq x)$是高斯分布的累积分布函数。